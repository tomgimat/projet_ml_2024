{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1338b089-51e7-42f5-8df7-d3b256482eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76b8e96e-fb6b-4d5e-9e7b-50d45378d791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Distracted driving causes more deaths in Canad...\n",
      "1       Missouri politicians have made statements afte...\n",
      "2       Home Alone 2: Lost in New York is full of viol...\n",
      "3       But things took a turn for the worse when riot...\n",
      "4       It’s no secret that Epstein and Schiff share a...\n",
      "                              ...                        \n",
      "1259    More than four million calls to the taxman are...\n",
      "1260    More under-18s are being taken to court for se...\n",
      "1261    The Government’s much vaunted Help to Buy Isa ...\n",
      "1262    The late Robin Williams once called cocaine “G...\n",
      "1263    The late Robin Williams once called cocaine “G...\n",
      "Name: text, Length: 1264, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data_raw = pd.read_csv('G:\\\\Trucs fac\\\\MONTPELLIER\\\\2023-2024\\\\S2\\\\UEs\\\\Machine Learning\\\\projet_ml_2024\\\\data\\\\HAI817_Projet_train.csv')\n",
    "print(data_raw['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "394a425c-4f09-4a69-b123-aba540059c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "340e2755-4756-494b-af4e-d917fc857f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Tom-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f21b8ff-63ea-4e8f-81fd-2035106d40c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Tom-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aade18d2-1123-483a-8387-680b60da68eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Tom-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# LEMMATISATION\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29738d25-0e20-40c2-bb8c-22bd53b75779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VECTORISATION TF-IDF \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def vectorize_text_tfidf(texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    return vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8ecd246-6f20-436d-8983-923a0fc45952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appel des fonctions sur l'ensemble des textes\n",
    "def complete_preprocess(texts):\n",
    "    clean_texts = [clean_text(text) for text in texts]\n",
    "    tokenized_texts = [tokenize(text) for text in clean_texts]\n",
    "    filtered_texts = [remove_stopwords(tokens) for tokens in tokenized_texts]\n",
    "    lemmatized_texts = [\" \".join(lemmatize_tokens(tokens)) for tokens in filtered_texts]\n",
    "    vectorized_texts = vectorize_text_tfidf(lemmatized_texts)\n",
    "    return vectorized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea09b40b-0385-475c-871d-8624f8a24b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts_preprocessed = complete_preprocess(data_raw['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72d75abb-d9e6-4fcb-8550-ccd57eaeb09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1264, 29668)\n"
     ]
    }
   ],
   "source": [
    "print(all_texts_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50349166-b9ef-4ae9-86de-e4eebec4f1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
